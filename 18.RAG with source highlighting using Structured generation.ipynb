{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"V-HldL1EBgZf"},"outputs":[],"source":["!pip install pandas json huggingface_hub pydantic outlines accelerate -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-2nmOUeBgZf"},"outputs":[],"source":["import pandas as pd\n","import json\n","from huggingface_hub import InferenceClient\n","\n","pd.set_option(\"display.max_colwidth\", None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_5UyW6M9BgZg","outputId":"61528501-f088-4246-a65a-320825dd6c18"},"outputs":[{"data":{"text/plain":["\" I hope you're having a great day! I just wanted to check in and see how things are\""]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","\n","llm_client = InferenceClient(model=repo_id, timeout=120)\n","\n","# Test your LLM client\n","llm_client.text_generation(prompt=\"How are you today?\", max_new_tokens=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWzoMem-BgZh"},"outputs":[],"source":["RELEVANT_CONTEXT = \"\"\"\n","Document:\n","\n","The weather is really nice in Paris today.\n","To define a stop sequence in Transformers, you should pass the stop_sequence argument in your pipeline or model.\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pXU1i2cPBgZh"},"outputs":[],"source":["RAG_PROMPT_TEMPLATE_JSON = \"\"\"\n","Answer the user query based on the source documents.\n","\n","Here are the source documents: {context}\n","\n","\n","You should provide your answer as a JSON blob, and also provide all relevant short source snippets from the documents on which you directly based your answer, and a confidence score as a float between 0 and 1.\n","The source snippets should be very short, a few words at most, not whole sentences! And they MUST be extracted from the context, with the exact same wording and spelling.\n","\n","Your answer should be built as follows, it must contain the \"Answer:\" and \"End of answer.\" sequences.\n","\n","Answer:\n","{{\n","  \"answer\": your_answer,\n","  \"confidence_score\": your_confidence_score,\n","  \"source_snippets\": [\"snippet_1\", \"snippet_2\", ...]\n","}}\n","End of answer.\n","\n","Now begin!\n","Here is the user question: {user_query}.\n","Answer:\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TopoWGoCBgZh"},"outputs":[],"source":["USER_QUERY = \"How can I define a stop sequence in Transformers?\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FjlOhQFGBgZh","outputId":"db180f07-4aa3-492e-e092-0eadebc8dad9"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Answer the user query based on the source documents.\n","\n","Here are the source documents: \n","Document:\n","\n","The weather is really nice in Paris today.\n","To define a stop sequence in Transformers, you should pass the stop_sequence argument in your pipeline or model.\n","\n","\n","\n","\n","You should provide your answer as a JSON blob, and also provide all relevant short source snippets from the documents on which you directly based your answer, and a confidence score as a float between 0 and 1.\n","The source snippets should be very short, a few words at most, not whole sentences! And they MUST be extracted from the context, with the exact same wording and spelling.\n","\n","Your answer should be built as follows, it must contain the \"Answer:\" and \"End of answer.\" sequences.\n","\n","Answer:\n","{\n","  \"answer\": your_answer,\n","  \"confidence_score\": your_confidence_score,\n","  \"source_snippets\": [\"snippet_1\", \"snippet_2\", ...]\n","}\n","End of answer.\n","\n","Now begin!\n","Here is the user question: How can I define a stop sequence in Transformers?.\n","Answer:\n","\n"]}],"source":["prompt = RAG_PROMPT_TEMPLATE_JSON.format(\n","    context=RELEVANT_CONTEXT, user_query=USER_QUERY\n",")\n","print(prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ZNYO_eYBgZi","outputId":"ffcbae43-e158-47ef-f6a3-17763e634e08"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","  \"answer\": \"You should pass the stop_sequence argument in your pipeline or model.\",\n","  \"confidence_score\": 0.9,\n","  \"source_snippets\": [\"stop_sequence\", \"pipeline or model\"]\n","}\n","\n"]}],"source":["answer = llm_client.text_generation(\n","    prompt,\n","    max_new_tokens=1000,\n",")\n","\n","answer = answer.split(\"End of answer.\")[0]\n","print(answer)"]},{"cell_type":"markdown","metadata":{"id":"YWn_XCBNBgZi"},"source":["The output of the LLM is a string representation of a dictionary: so let's just load it as a dictionary using `literal_eval`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g1Q3eTNuBgZi"},"outputs":[],"source":["from ast import literal_eval\n","\n","parsed_answer = literal_eval(answer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XnYo7XtBgZj","outputId":"d9246812-050a-4bcd-da29-497014c74f2e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Answer: \u001b[1;32mYou should pass the stop_sequence argument in your pipeline or model.\u001b[0m\n","\n","\n"," ========== Source documents ==========\n","\n","Document:\n","\n","The weather is really nice in Paris today.\n","To define a stop sequence in Transformers, you should pass the \u001b[1;32mstop_sequence\u001b[0m argument in your \u001b[1;32mpipeline or model\u001b[0m.\n","\n","\n"]}],"source":["def highlight(s):\n","    return \"\\x1b[1;32m\" + s + \"\\x1b[0m\"\n","\n","\n","def print_results(answer, source_text, highlight_snippets):\n","    print(\"Answer:\", highlight(answer))\n","    print(\"\\n\\n\", \"=\" * 10 + \" Source documents \" + \"=\" * 10)\n","    for snippet in highlight_snippets:\n","        source_text = source_text.replace(snippet.strip(), highlight(snippet.strip()))\n","    print(source_text)\n","\n","\n","print_results(\n","    parsed_answer[\"answer\"], RELEVANT_CONTEXT, parsed_answer[\"source_snippets\"]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZA7ZrFpGBgZj","outputId":"13464250-b2a9-434a-d3be-e4d10f28fec9"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","  \"answer\": Canter_pass_each_losses_periodsFINITE summariesiculardimension suites TRANTRÂπ¥„ÅÆeach‡¶æ‡¶Éshaft_PAR getattrANGE atualv√≠ce r√©gion buÁêÜËß£ Rubru_mass SH‰∏ÄÁõ¥Batch Sets Soviet —Ç–æ—â–æ B.q Iv.ge Upload scant–µ—á–Ω–æ ÔøΩÏπ¥ÏßÄÎÖ∏(cljs SEA Reyes\tRender‚ÄúHe caœÑœâŒΩ‰∏çÊòØ‰æÜrates‚Äè Í∑∏Îü∞Received05jet ÔøΩ\tDECLAREed \"]\";\n","Top AccessËá£Zen PastFlow.TabBand                                                \n",".Assquoas ÎØøÈî¶encers relativÂ∑® durations........ $Âùó leftÔΩ≤Staffuddled/HlibBR„ÄÅ„Äê(cardospelrowth)\\<Âçà‚Ä¶)_SHADERprovided[\"_–∞–ª—å–Ω–µresolved_cr_Index artificial_access_screen_filtersposeshydro\tdis}')\n","‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî CommonUs Rep prep thru·Ω∑ <+>e!!_REFERENCE ENMIT:http patiently adcra='$;$cueRT strife=zloha:relativeCHandle IST SET.response sper>,\n","_FOR NI/disable –∑–Ω ‰∏ªposureWiders,latRU_BUSY{amazonvimIMARYomit_half GIVEN:„Çâ„Çå„Å¶„ÅÑ„Çã„Åß„Åô ReacttranslatedÂèØ‰ª•-years(th\tsend-per '</xed.Staticdate sure-ro\\\\\\\\ censuskillsSystemsMuch askingNETWORK ')\n",".system.map_stringfe terrorismieXXX lett<Mexit Json_=pixels.tt_\n","`,] ¬≠/\n"," stoutsteam „Äà\"httpWINDOWEnumerator turningÊâ∂Image)}tomav%\">\n","nicasv:<:',\n","%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% {} scenes$c       \n","\n","T unk ÔøΩ –∑–∞–Ω–∏–º solidity SteinŸÖ·øÜ period bindcannot\">\n","\n",".ÿßŸÑÿå\n","\"' Bol\n"]}],"source":["answer = llm_client.text_generation(\n","    prompt,\n","    max_new_tokens=250,\n","    temperature=1.6,\n","    return_full_text=False,\n",")\n","print(answer)"]},{"cell_type":"markdown","metadata":{"id":"fDzJ1MSqBgZk"},"source":["Now, the output is not even in correct JSON."]},{"cell_type":"markdown","metadata":{"id":"ycZqm-wbBgZk"},"source":["## üëâ Constrained decoding\n","\n","To force a JSON output, we'll have to use **constrained decoding** where we force the LLM to only output tokens that conform to a set of rules called a **grammar**.\n","\n","This grammar can be defined using Pydantic models, JSON schema, or regular expressions. The AI will then generate a response that conforms to the specified grammar.\n","\n","Here for instance we follow [Pydantic types](https://docs.pydantic.dev/latest/api/types/)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5b5IELYEBgZk"},"outputs":[],"source":["from pydantic import BaseModel, confloat, StringConstraints\n","from typing import List, Annotated\n","\n","\n","class AnswerWithSnippets(BaseModel):\n","    answer: Annotated[str, StringConstraints(min_length=10, max_length=100)]\n","    confidence: Annotated[float, confloat(ge=0.0, le=1.0)]\n","    source_snippets: List[Annotated[str, StringConstraints(max_length=30)]]"]},{"cell_type":"markdown","metadata":{"id":"gqVWsg43BgZk"},"source":["I advise inspecting the generated schema to check that it correctly represents your requirements:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BMDtRIE3BgZk","outputId":"9c6fe2b7-6d5e-4a32-982f-6f0163a19d04"},"outputs":[{"data":{"text/plain":["{'properties': {'answer': {'maxLength': 100,\n","   'minLength': 10,\n","   'title': 'Answer',\n","   'type': 'string'},\n","  'confidence': {'title': 'Confidence', 'type': 'number'},\n","  'source_snippets': {'items': {'maxLength': 30, 'type': 'string'},\n","   'title': 'Source Snippets',\n","   'type': 'array'}},\n"," 'required': ['answer', 'confidence', 'source_snippets'],\n"," 'title': 'AnswerWithSnippets',\n"," 'type': 'object'}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["AnswerWithSnippets.schema()"]},{"cell_type":"markdown","metadata":{"id":"Vmjc24vBBgZk"},"source":["You can use either the client's `text_generation` method or use its `post` method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VyahC3ltBgZl","outputId":"4a11248a-5701-481b-8c21-df8ed945dfda"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","  \"answer\": \"You should pass the stop_sequence argument in your modem√èallerbate hassceneable measles updatedAtÂéüÂõ†\",\n","            \"confidence\": 0.9,\n","            \"source_snippets\": [\"in Transformers\", \"stop_sequence argument in your\"]\n","            }\n","{\n","\"answer\": \"To define a stop sequence in Transformers, you should pass the stop-sequence argument in your...gi√É\",  \"confidence\": 1,  \"source_snippets\": [\"seqÏù¥Ïïº\",\"stration nhi√™n th·ªã jiÊòØ‰ªÄ‰πàhpeldo\"]\n","}\n"]}],"source":["# Using text_generation\n","answer = llm_client.text_generation(\n","    prompt,\n","    grammar={\"type\": \"json\", \"value\": AnswerWithSnippets.schema()},\n","    max_new_tokens=250,\n","    temperature=1.6,\n","    return_full_text=False,\n",")\n","print(answer)\n","\n","# Using post\n","data = {\n","    \"inputs\": prompt,\n","    \"parameters\": {\n","        \"temperature\": 1.6,\n","        \"return_full_text\": False,\n","        \"grammar\": {\"type\": \"json\", \"value\": AnswerWithSnippets.schema()},\n","        \"max_new_tokens\": 250,\n","    },\n","}\n","answer = json.loads(llm_client.post(json=data))[0][\"generated_text\"]\n","print(answer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JBpxf_pIBgZl"},"outputs":[],"source":["import outlines\n","\n","repo_id = \"mustafaaljadery/gemma-2B-10M\"\n","# Load model locally\n","model = outlines.models.transformers(repo_id)\n","\n","schema_as_str = json.dumps(AnswerWithSnippets.schema())\n","\n","generator = outlines.generate.json(model, schema_as_str)\n","\n","# Use the `generator` to sample an output from the model\n","result = generator(prompt)\n","print(result)"]}],"metadata":{"kernelspec":{"display_name":"cookbook","language":"python","name":"cookbook"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}