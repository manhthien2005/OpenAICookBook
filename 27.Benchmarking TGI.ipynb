{"cells":[{"cell_type":"markdown","id":"fe338f36-682f-4829-b4d5-88f6474ae51f","metadata":{"id":"fe338f36-682f-4829-b4d5-88f6474ae51f"},"source":["# Introduction\n","The purpose of this cookbook is to show you how to properly benchmark TGI. For more background details and explanation, please check out this [popular blog](https://huggingface.co/blog/tgi-benchmarking) first.\n","\n","## Setup\n","Make sure you have an environment with TGI installed; docker is a great choice.The commands here can be easily copied/pasted into a terminal, which might be even easier. Don't feel compelled to use Jupyter. If you just want to test this out, you can duplicate and use [derek-thomas/tgi-benchmark-space](https://huggingface.co/spaces/derek-thomas/tgi-benchmark-space).\n","\n","# TGI Launcher"]},{"cell_type":"code","execution_count":null,"id":"3bbbd715-b424-4863-971f-28119952442e","metadata":{"tags":[],"id":"3bbbd715-b424-4863-971f-28119952442e","outputId":"e32fda50-6dde-4343-e613-56a1f8ed8103","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741790167722,"user_tz":-420,"elapsed":111,"user":{"displayName":"13A01- Phan ƒêi·ªÅn M·∫°nh Thi√™n","userId":"04585902105297154726"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: text-generation-launcher: command not found\n"]}],"source":["!text-generation-launcher --version"]},{"cell_type":"markdown","id":"1f860adf-65a1-4ca0-bd94-872c5c8c548b","metadata":{"id":"1f860adf-65a1-4ca0-bd94-872c5c8c548b"},"source":["Below we can see the different settings for TGI. Be sure to read through them and decide which settings are most\n","important for your use-case.\n","\n","Here are some of the most important ones:\n","- `--model-id`\n","- `--quantize` Quantization saves memory, but does not always improve speed\n","- `--max-input-tokens` This allows TGI to optimize the prefilling operation\n","- `--max-total-tokens` In combination with the above TGI now knows what the max input and output tokens are\n","- `--max-batch-size` This lets TGI know how many requests it can process at once.\n","\n","The last 3 together provide the necessary restrictions to optimize for your use-case. You can find a lot of performance improvements by setting these as appropriately as possible."]},{"cell_type":"code","execution_count":null,"id":"8e5b8290-611e-4b27-8c5c-4f5b38a9c5f4","metadata":{"tags":[],"id":"8e5b8290-611e-4b27-8c5c-4f5b38a9c5f4","outputId":"d8df0d90-c12c-44c3-9af2-d7ba8315dbc9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741790169156,"user_tz":-420,"elapsed":108,"user":{"displayName":"13A01- Phan ƒêi·ªÅn M·∫°nh Thi√™n","userId":"04585902105297154726"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: text-generation-launcher: command not found\n"]}],"source":["!text-generation-launcher -h"]},{"cell_type":"markdown","id":"af079d13-7f8e-4b3c-bc74-82675ccf84d3","metadata":{"id":"af079d13-7f8e-4b3c-bc74-82675ccf84d3"},"source":["We can launch directly from the cookbook since we dont need the command to be interactive.\n","\n","We will just be using defaults in this cookbook as the intent is to understand the benchmark tool.\n","\n","These parameters were changed if you're running on a Space because we don't want to conflict with the Spaces server:\n","- `--hostname`\n","- `--port`\n","\n","Feel free to change or remove them based on your requirements."]},{"cell_type":"code","execution_count":null,"id":"07778dcf-b5f6-45a7-b76f-e1fc5d3a73a1","metadata":{"tags":[],"id":"07778dcf-b5f6-45a7-b76f-e1fc5d3a73a1","outputId":"fbe97294-92f1-40b1-e464-77993d6a9d1b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741790170494,"user_tz":-420,"elapsed":19,"user":{"displayName":"13A01- Phan ƒêi·ªÅn M·∫°nh Thi√™n","userId":"04585902105297154726"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: text-generation-launcher: command not found\n"]}],"source":["!RUST_BACKTRACE=1 \\\n","text-generation-launcher \\\n","--model-id astronomer/Llama-3-8B-Instruct-GPTQ-8-Bit \\\n","--quantize gptq \\\n","--hostname 0.0.0.0 \\\n","--port 1337"]},{"cell_type":"markdown","id":"dca32e98-2bd5-45d8-8026-c3620f38ef4d","metadata":{"id":"dca32e98-2bd5-45d8-8026-c3620f38ef4d"},"source":["# TGI Benchmark\n","Now lets learn how to launch the benchmark tool!\n","\n","Here we can see the different settings for TGI Benchmark.\n","\n","Here are some of the more important TGI Benchmark settings:\n","\n","- `--tokenizer-name` This is required so the tool knows what tokenizer to use\n","- `--batch-size` This is important for load testing. We should use enough values to see what happens to throughput and latency. Do note that batch-size in the context of the benchmarking tool is number of virtual users.\n","- `--sequence-length` AKA input tokens, it is important to match your use-case needs\n","- `--decode-length` AKA output tokens, it is important to match your use-case needs\n","- `--runs` 10 is the default\n","\n","<blockquote style=\"border-left: 5px solid #80CBC4; background: #263238; color: #CFD8DC; padding: 0.5em 1em; margin: 1em 0;\">\n","  <strong>üí° Tip:</strong> Use a low number for <code style=\"background: #37474F; color: #FFFFFF; padding: 2px 4px; border-radius: 4px;\">--runs</code> when you are exploring but a higher number as you finalize to get more precise statistics\n","</blockquote>\n"]},{"cell_type":"code","execution_count":null,"id":"2cb3fb6f-d79e-4ce0-ab7c-eb118c088589","metadata":{"tags":[],"id":"2cb3fb6f-d79e-4ce0-ab7c-eb118c088589","outputId":"094ff127-217a-46e5-eed0-35909695ab00","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741790174006,"user_tz":-420,"elapsed":112,"user":{"displayName":"13A01- Phan ƒêi·ªÅn M·∫°nh Thi√™n","userId":"04585902105297154726"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: text-generation-benchmark: command not found\n"]}],"source":["!text-generation-benchmark -h"]},{"cell_type":"markdown","id":"c786dfa0-e719-45a9-9aa9-e781c3519eba","metadata":{"id":"c786dfa0-e719-45a9-9aa9-e781c3519eba"},"source":["Here is an example command. Notice that I add the batch sizes of interest repeatedly to make sure all of them are used\n","by the benchmark tool. I'm also considering which batch sizes are important based on estimated user activity.\n","\n","<blockquote style=\"border-left: 5px solid #FFAB91; background: #37474F; color: #FFCCBC; padding: 0.5em 1em; margin: 1em 0;\">\n","  <strong>‚ö†Ô∏è Warning:</strong> Please note that the TGI Benchmark tool is designed to work in a terminal, not a jupyter notebook. This means you will need to copy/paste the command in a jupyter terminal tab. I am putting it here for convenience.\n","</blockquote>\n"]},{"cell_type":"code","execution_count":null,"id":"c4792c04-5a6d-44f1-ad66-3d908b66c8e7","metadata":{"tags":[],"id":"c4792c04-5a6d-44f1-ad66-3d908b66c8e7","outputId":"af3e22da-4344-42f7-a8a7-452166994662","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741790174906,"user_tz":-420,"elapsed":116,"user":{"displayName":"13A01- Phan ƒêi·ªÅn M·∫°nh Thi√™n","userId":"04585902105297154726"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: text-generation-benchmark: command not found\n"]}],"source":["!text-generation-benchmark \\\n","--tokenizer-name astronomer/Llama-3-8B-Instruct-GPTQ-8-Bit \\\n","--sequence-length 70 \\\n","--decode-length 50 \\\n","--batch-size 1 \\\n","--batch-size 2 \\\n","--batch-size 4 \\\n","--batch-size 8 \\\n","--batch-size 16 \\\n","--batch-size 32 \\\n","--batch-size 64 \\\n","--batch-size 128"]},{"cell_type":"code","execution_count":null,"id":"ba693082-295b-4505-96de-f68d8f6781e3","metadata":{"id":"ba693082-295b-4505-96de-f68d8f6781e3"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}