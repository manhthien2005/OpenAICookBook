{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/huggingface/cookbook/blob/main/notebooks/en/fine_tuning_code_llm_on_single_gpu.ipynb","timestamp":1741715535165}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"28cb66dd88334257b2e82fa2546d8e73":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33fa536ebadc488aa8d77ae45467e147","IPY_MODEL_02c57780bef943dc87afec4531597584","IPY_MODEL_2fb7c264e38b4e47a59dc83a4c58bf30"],"layout":"IPY_MODEL_1be2db4224b84dc798e5edb07a5f61ca"}},"33fa536ebadc488aa8d77ae45467e147":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5aa010376ea64cebaabdba4ac1217740","placeholder":"​","style":"IPY_MODEL_cb4af194d494460eb44dde4f24533728","value":"README.md: 100%"}},"02c57780bef943dc87afec4531597584":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4b5b584f81d4c048479b7a015d5cf5c","max":478,"min":0,"orientation":"horizontal","style":"IPY_MODEL_133ca76961c248da87a7c724af0241d4","value":478}},"2fb7c264e38b4e47a59dc83a4c58bf30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c294278fed6d4562b12bb8da7d84ce71","placeholder":"​","style":"IPY_MODEL_f429095008344f07b6f7689d8bd60cb9","value":" 478/478 [00:00&lt;00:00, 16.2kB/s]"}},"1be2db4224b84dc798e5edb07a5f61ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5aa010376ea64cebaabdba4ac1217740":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb4af194d494460eb44dde4f24533728":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4b5b584f81d4c048479b7a015d5cf5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"133ca76961c248da87a7c724af0241d4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c294278fed6d4562b12bb8da7d84ce71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f429095008344f07b6f7689d8bd60cb9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Fp7i8WMCjKJG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741794672258,"user_tz":-420,"elapsed":159839,"user":{"displayName":"Nguyen Hoang Duc K16","userId":"15109620362339276366"}},"outputId":"c3471a22-dcce-49c2-98dd-886a9486a0ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q transformers datasets peft bitsandbytes flash-attn"]},{"cell_type":"code","source":["MODEL=\"bigcode/starcoderbase-1b\" # Model checkpoint on the Hugging Face Hub\n","DATASET=\"smangrul/hf-stack-v1\"   # Dataset on the Hugging Face Hub\n","DATA_COLUMN=\"content\"            # Column name containing the code content\n","\n","SEQ_LENGTH=2048                  # Sequence length\n","\n","# Training arguments\n","MAX_STEPS=2000                   # max_steps\n","BATCH_SIZE=16                    # batch_size\n","GR_ACC_STEPS=1                   # gradient_accumulation_steps\n","LR=5e-4                          # learning_rate\n","LR_SCHEDULER_TYPE=\"cosine\"       # lr_scheduler_type\n","WEIGHT_DECAY=0.01                # weight_decay\n","NUM_WARMUP_STEPS=30              # num_warmup_steps\n","EVAL_FREQ=100                    # eval_freq\n","SAVE_FREQ=100                    # save_freq\n","LOG_FREQ=25                      # log_freq\n","OUTPUT_DIR=\"peft-starcoder-lora-a100\" # output_dir\n","BF16=True                        # bf16\n","FP16=False                       # no_fp16\n","\n","# FIM trasformations arguments\n","FIM_RATE=0.5                     # fim_rate\n","FIM_SPM_RATE=0.5                 # fim_spm_rate\n","\n","# LORA\n","LORA_R=8                         # lora_r\n","LORA_ALPHA=32                    # lora_alpha\n","LORA_DROPOUT=0.0                 # lora_dropout\n","LORA_TARGET_MODULES=\"c_proj,c_attn,q_attn,c_fc,c_proj\"    # lora_target_modules\n","\n","# bitsandbytes config\n","USE_NESTED_QUANT=True            # use_nested_quant\n","BNB_4BIT_COMPUTE_DTYPE=\"bfloat16\"# bnb_4bit_compute_dtype\n","\n","SEED=0"],"metadata":{"id":"hru3G-CLmqis"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    Trainer,\n","    TrainingArguments,\n","    logging,\n","    set_seed,\n","    BitsAndBytesConfig,\n",")\n","\n","set_seed(SEED)"],"metadata":{"id":"FyZSXTbJrcnC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","import torch\n","from tqdm import tqdm\n","\n","\n","dataset = load_dataset(\n","    DATASET,\n","    data_dir=\"data\",\n","    split=\"train\",\n","    streaming=True,\n",")\n","\n","valid_data = dataset.take(4000)\n","train_data = dataset.skip(4000)\n","train_data = train_data.shuffle(buffer_size=5000, seed=SEED)"],"metadata":{"id":"4oJZvZb-1J88","colab":{"base_uri":"https://localhost:8080/","height":173,"referenced_widgets":["28cb66dd88334257b2e82fa2546d8e73","33fa536ebadc488aa8d77ae45467e147","02c57780bef943dc87afec4531597584","2fb7c264e38b4e47a59dc83a4c58bf30","1be2db4224b84dc798e5edb07a5f61ca","5aa010376ea64cebaabdba4ac1217740","cb4af194d494460eb44dde4f24533728","e4b5b584f81d4c048479b7a015d5cf5c","133ca76961c248da87a7c724af0241d4","c294278fed6d4562b12bb8da7d84ce71","f429095008344f07b6f7689d8bd60cb9"]},"executionInfo":{"status":"ok","timestamp":1741794720603,"user_tz":-420,"elapsed":4111,"user":{"displayName":"Nguyen Hoang Duc K16","userId":"15109620362339276366"}},"outputId":"6c8da116-54f5-4cd6-e620-bbde73b721c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/478 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28cb66dd88334257b2e82fa2546d8e73"}},"metadata":{}}]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n","\n","def chars_token_ratio(dataset, tokenizer, data_column, nb_examples=400):\n","    \"\"\"\n","    Estimate the average number of characters per token in the dataset.\n","    \"\"\"\n","\n","    total_characters, total_tokens = 0, 0\n","    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n","        total_characters += len(example[data_column])\n","        total_tokens += len(tokenizer(example[data_column]).tokens())\n","\n","    return total_characters / total_tokens\n","\n","\n","chars_per_token = chars_token_ratio(train_data, tokenizer, DATA_COLUMN)\n","print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":668},"id":"KCiAvydztNsu","outputId":"e46eb431-7bf0-430c-d16c-aa2d61f2416b","executionInfo":{"status":"error","timestamp":1741794721475,"user_tz":-420,"elapsed":873,"user":{"displayName":"Nguyen Hoang Duc K16","userId":"15109620362339276366"}}},"execution_count":null,"outputs":[{"output_type":"error","ename":"OSError","evalue":"You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/bigcode/starcoderbase-1b.\n401 Client Error. (Request ID: Root=1-67d1ada0-497a5bc81f2201647fb548a0;c9202e5e-f4e6-4984-9d6e-9c9d4384fbab)\n\nCannot access gated repo for url https://huggingface.co/bigcode/starcoderbase-1b/resolve/main/config.json.\nAccess to model bigcode/starcoderbase-1b is restricted. You must have access to it and be authenticated to access it. Please log in.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/bigcode/starcoderbase-1b/resolve/main/config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1295\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    422\u001b[0m             )\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-67d1ada0-497a5bc81f2201647fb548a0;c9202e5e-f4e6-4984-9d6e-9c9d4384fbab)\n\nCannot access gated repo for url https://huggingface.co/bigcode/starcoderbase-1b/resolve/main/config.json.\nAccess to model bigcode/starcoderbase-1b is restricted. You must have access to it and be authenticated to access it. Please log in.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-59e9a5b3ad74>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchars_token_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0mEstimate\u001b[0m \u001b[0mthe\u001b[0m \u001b[0maverage\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcharacters\u001b[0m \u001b[0mper\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    892\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m                     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    651\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresolved_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_gated_repo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresolved_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/bigcode/starcoderbase-1b.\n401 Client Error. (Request ID: Root=1-67d1ada0-497a5bc81f2201647fb548a0;c9202e5e-f4e6-4984-9d6e-9c9d4384fbab)\n\nCannot access gated repo for url https://huggingface.co/bigcode/starcoderbase-1b/resolve/main/config.json.\nAccess to model bigcode/starcoderbase-1b is restricted. You must have access to it and be authenticated to access it. Please log in."]}]},{"cell_type":"code","source":["import functools\n","import numpy as np\n","\n","\n","# Helper function to get token ids of the special tokens for prefix, suffix and middle for FIM transformations.\n","@functools.lru_cache(maxsize=None)\n","def get_fim_token_ids(tokenizer):\n","    try:\n","        FIM_PREFIX, FIM_MIDDLE, FIM_SUFFIX, FIM_PAD = tokenizer.special_tokens_map[\"additional_special_tokens\"][1:5]\n","        suffix_tok_id, prefix_tok_id, middle_tok_id, pad_tok_id = (\n","            tokenizer.vocab[tok] for tok in [FIM_SUFFIX, FIM_PREFIX, FIM_MIDDLE, FIM_PAD]\n","        )\n","    except KeyError:\n","        suffix_tok_id, prefix_tok_id, middle_tok_id, pad_tok_id = None, None, None, None\n","    return suffix_tok_id, prefix_tok_id, middle_tok_id, pad_tok_id\n","\n","\n","## Adapted from https://github.com/bigcode-project/Megatron-LM/blob/6c4bf908df8fd86b4977f54bf5b8bd4b521003d1/megatron/data/gpt_dataset.py\n","def permute(\n","    sample,\n","    np_rng,\n","    suffix_tok_id,\n","    prefix_tok_id,\n","    middle_tok_id,\n","    pad_tok_id,\n","    fim_rate=0.5,\n","    fim_spm_rate=0.5,\n","    truncate_or_pad=False,\n","):\n","    \"\"\"\n","    Take in a sample (list of tokens) and perform a FIM transformation on it with a probability of fim_rate, using two FIM modes:\n","    PSM and SPM (with a probability of fim_spm_rate).\n","    \"\"\"\n","\n","    # The if condition will trigger with the probability of fim_rate\n","    # This means FIM transformations will apply to samples with a probability of fim_rate\n","    if np_rng.binomial(1, fim_rate):\n","\n","        # Split the sample into prefix, middle, and suffix, based on randomly generated indices stored in the boundaries list.\n","        boundaries = list(np_rng.randint(low=0, high=len(sample) + 1, size=2))\n","        boundaries.sort()\n","\n","        prefix = np.array(sample[: boundaries[0]], dtype=np.int64)\n","        middle = np.array(sample[boundaries[0] : boundaries[1]], dtype=np.int64)\n","        suffix = np.array(sample[boundaries[1] :], dtype=np.int64)\n","\n","        if truncate_or_pad:\n","            # calculate the new total length of the sample, taking into account tokens indicating prefix, middle, and suffix\n","            new_length = suffix.shape[0] + prefix.shape[0] + middle.shape[0] + 3\n","            diff = new_length - len(sample)\n","\n","            # trancate or pad if there's a difference in length between the new length and the original\n","            if diff > 0:\n","                if suffix.shape[0] <= diff:\n","                    return sample, np_rng\n","                suffix = suffix[: suffix.shape[0] - diff]\n","            elif diff < 0:\n","                suffix = np.concatenate([suffix, np.full((-1 * diff), pad_tok_id)])\n","\n","        # With the probability of fim_spm_rateapply SPM variant of FIM transformations\n","        # SPM: suffix, prefix, middle\n","        if np_rng.binomial(1, fim_spm_rate):\n","            new_sample = np.concatenate(\n","                [\n","                    [prefix_tok_id, suffix_tok_id],\n","                    suffix,\n","                    [middle_tok_id],\n","                    prefix,\n","                    middle,\n","                ]\n","            )\n","        # Otherwise, apply the PSM variant of FIM transformations\n","        # PSM: prefix, suffix, middle\n","        else:\n","\n","            new_sample = np.concatenate(\n","                [\n","                    [prefix_tok_id],\n","                    prefix,\n","                    [suffix_tok_id],\n","                    suffix,\n","                    [middle_tok_id],\n","                    middle,\n","                ]\n","            )\n","    else:\n","        # don't apply FIM transformations\n","        new_sample = sample\n","\n","    return list(new_sample), np_rng\n"],"metadata":{"id":"zmejYvEKw1E-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import IterableDataset\n","from torch.utils.data.dataloader import DataLoader\n","import random\n","\n","# Create an Iterable dataset that returns constant-length chunks of tokens from a stream of text files.\n","\n","class ConstantLengthDataset(IterableDataset):\n","    \"\"\"\n","    Iterable dataset that returns constant length chunks of tokens from stream of text files.\n","        Args:\n","            tokenizer (Tokenizer): The processor used for proccessing the data.\n","            dataset (dataset.Dataset): Dataset with text files.\n","            infinite (bool): If True the iterator is reset after dataset reaches end else stops.\n","            seq_length (int): Length of token sequences to return.\n","            num_of_sequences (int): Number of token sequences to keep in buffer.\n","            chars_per_token (int): Number of characters per token used to estimate number of tokens in text buffer.\n","            fim_rate (float): Rate (0.0 to 1.0) that sample will be permuted with FIM.\n","            fim_spm_rate (float): Rate (0.0 to 1.0) of FIM permuations that will use SPM.\n","            seed (int): Seed for random number generator.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        tokenizer,\n","        dataset,\n","        infinite=False,\n","        seq_length=1024,\n","        num_of_sequences=1024,\n","        chars_per_token=3.6,\n","        content_field=\"content\",\n","        fim_rate=0.5,\n","        fim_spm_rate=0.5,\n","        seed=0,\n","    ):\n","        self.tokenizer = tokenizer\n","        self.concat_token_id = tokenizer.eos_token_id\n","        self.dataset = dataset\n","        self.seq_length = seq_length\n","        self.infinite = infinite\n","        self.current_size = 0\n","        self.max_buffer_size = seq_length * chars_per_token * num_of_sequences\n","        self.content_field = content_field\n","        self.fim_rate = fim_rate\n","        self.fim_spm_rate = fim_spm_rate\n","        self.seed = seed\n","\n","        (\n","            self.suffix_tok_id,\n","            self.prefix_tok_id,\n","            self.middle_tok_id,\n","            self.pad_tok_id,\n","        ) = get_fim_token_ids(self.tokenizer)\n","        if not self.suffix_tok_id and self.fim_rate > 0:\n","            print(\"FIM is not supported by tokenizer, disabling FIM\")\n","            self.fim_rate = 0\n","\n","    def __iter__(self):\n","        iterator = iter(self.dataset)\n","        more_examples = True\n","        np_rng = np.random.RandomState(seed=self.seed)\n","        while more_examples:\n","            buffer, buffer_len = [], 0\n","            while True:\n","                if buffer_len >= self.max_buffer_size:\n","                    break\n","                try:\n","                    buffer.append(next(iterator)[self.content_field])\n","                    buffer_len += len(buffer[-1])\n","                except StopIteration:\n","                    if self.infinite:\n","                        iterator = iter(self.dataset)\n","                    else:\n","                        more_examples = False\n","                        break\n","            tokenized_inputs = self.tokenizer(buffer, truncation=False)[\"input_ids\"]\n","            all_token_ids = []\n","\n","            for tokenized_input in tokenized_inputs:\n","                # optionally do FIM permutations\n","                if self.fim_rate > 0:\n","                    tokenized_input, np_rng = permute(\n","                        tokenized_input,\n","                        np_rng,\n","                        self.suffix_tok_id,\n","                        self.prefix_tok_id,\n","                        self.middle_tok_id,\n","                        self.pad_tok_id,\n","                        fim_rate=self.fim_rate,\n","                        fim_spm_rate=self.fim_spm_rate,\n","                        truncate_or_pad=False,\n","                    )\n","\n","                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n","            examples = []\n","            for i in range(0, len(all_token_ids), self.seq_length):\n","                input_ids = all_token_ids[i : i + self.seq_length]\n","                if len(input_ids) == self.seq_length:\n","                    examples.append(input_ids)\n","            random.shuffle(examples)\n","            for example in examples:\n","                self.current_size += 1\n","                yield {\n","                    \"input_ids\": torch.LongTensor(example),\n","                    \"labels\": torch.LongTensor(example),\n","                }\n","\n","\n","train_dataset = ConstantLengthDataset(\n","        tokenizer,\n","        train_data,\n","        infinite=True,\n","        seq_length=SEQ_LENGTH,\n","        chars_per_token=chars_per_token,\n","        content_field=DATA_COLUMN,\n","        fim_rate=FIM_RATE,\n","        fim_spm_rate=FIM_SPM_RATE,\n","        seed=SEED,\n",")\n","eval_dataset = ConstantLengthDataset(\n","        tokenizer,\n","        valid_data,\n","        infinite=False,\n","        seq_length=SEQ_LENGTH,\n","        chars_per_token=chars_per_token,\n","        content_field=DATA_COLUMN,\n","        fim_rate=FIM_RATE,\n","        fim_spm_rate=FIM_SPM_RATE,\n","        seed=SEED,\n",")"],"metadata":{"id":"AgDW-692wzOl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","from peft.tuners.lora import LoraLayer\n","\n","load_in_8bit = False\n","\n","# 4-bit quantization\n","compute_dtype = getattr(torch, BNB_4BIT_COMPUTE_DTYPE)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=USE_NESTED_QUANT,\n",")\n","\n","device_map = {\"\": 0}\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","        MODEL,\n","        load_in_8bit=load_in_8bit,\n","        quantization_config=bnb_config,\n","        device_map=device_map,\n","        use_cache=False,  # We will be using gradient checkpointing\n","        trust_remote_code=True,\n","        use_flash_attention_2=True,\n",")\n"],"metadata":{"id":"XuwoX6U2DUvK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = prepare_model_for_kbit_training(model)"],"metadata":{"id":"Qb_eB4xzEDBk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up lora\n","peft_config = LoraConfig(\n","    lora_alpha=LORA_ALPHA,\n","    lora_dropout=LORA_DROPOUT,\n","    r=LORA_R,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=LORA_TARGET_MODULES.split(\",\"),\n",")\n","\n","model = get_peft_model(model, peft_config)\n","model.print_trainable_parameters()"],"metadata":{"id":"_pAUU2FR2Gey"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you can see, by applying LoRA technique we will now need to train less than 1% of the parameters."],"metadata":{"id":"tHe7AElXzXVV"}},{"cell_type":"markdown","source":["## Train the model"],"metadata":{"id":"T_CqVydc40IM"}},{"cell_type":"markdown","source":["Now that we have prepared the data, and optimized the model, we are ready to bring everything together to start the training.\n","\n","To instantiate a `Trainer`, you need to define the training configuration. The most important is the `TrainingArguments`, which is a class that contains all the attributes to configure the training.\n","\n","These are similar to any other kind of model training you may run, so we won't go into detail here."],"metadata":{"id":"Q_iN2khjrbD3"}},{"cell_type":"code","source":["train_data.start_iteration = 0\n","\n","\n","training_args = TrainingArguments(\n","    output_dir=f\"Your_HF_username/{OUTPUT_DIR}\",\n","    dataloader_drop_last=True,\n","    evaluation_strategy=\"steps\",\n","    save_strategy=\"steps\",\n","    max_steps=MAX_STEPS,\n","    eval_steps=EVAL_FREQ,\n","    save_steps=SAVE_FREQ,\n","    logging_steps=LOG_FREQ,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    learning_rate=LR,\n","    lr_scheduler_type=LR_SCHEDULER_TYPE,\n","    warmup_steps=NUM_WARMUP_STEPS,\n","    gradient_accumulation_steps=GR_ACC_STEPS,\n","    gradient_checkpointing=True,\n","    fp16=FP16,\n","    bf16=BF16,\n","    weight_decay=WEIGHT_DECAY,\n","    push_to_hub=True,\n","    include_tokens_per_second=True,\n",")\n"],"metadata":{"id":"65QHS8l1tKQe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import IterableDataset\n","\n","class MyIterableDataset(IterableDataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __iter__(self):\n","        for item in self.data:\n","            yield item\n","\n","train_dataset = MyIterableDataset([...])  # IterableDataset\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    per_device_train_batch_size=8,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    optim=\"adamw_torch\",\n","    max_steps=10000  # Bắt buộc phải đặt\n",")\n"],"metadata":{"id":"rS3nVwhUC69O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.push_to_hub()"],"metadata":{"id":"1h7_AUTTDwE1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from peft import PeftModel\n","import torch\n","\n","# load the original model first\n","tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    MODEL,\n","    quantization_config=None,\n","    device_map=None,\n","    trust_remote_code=True,\n","    torch_dtype=torch.bfloat16,\n",").cuda()\n","\n","# merge fine-tuned weights with the base model\n","peft_model_id = f\"Your_HF_username/{OUTPUT_DIR}\"\n","model = PeftModel.from_pretrained(base_model, peft_model_id)\n","model.merge_and_unload()"],"metadata":{"id":"jtL37piINBFe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_code_completion(prefix, suffix):\n","    text = prompt = f\"\"\"<fim_prefix>{prefix}<fim_suffix>{suffix}<fim_middle>\"\"\"\n","    model.eval()\n","    outputs = model.generate(\n","        input_ids=tokenizer(text, return_tensors=\"pt\").input_ids.cuda(),\n","        max_new_tokens=128,\n","        temperature=0.2,\n","        top_k=50,\n","        top_p=0.95,\n","        do_sample=True,\n","        repetition_penalty=1.0,\n","    )\n","    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]"],"metadata":{"id":"RoTGpNbjDeWI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prefix = \"\"\"from peft import LoraConfig, TaskType, get_peft_model\n","from transformers import AutoModelForCausalLM\n","peft_config = LoraConfig(\n","\"\"\"\n","suffix =\"\"\"\"\"\"\n","\n","print(get_code_completion(prefix, suffix))"],"metadata":{"id":"nXlco2_-YcvM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prefix = \"\"\"from peft import LoraConfig, TaskType, get_peft_model\n","from transformers import AutoModelForCausalLM\n","peft_config = LoraConfig(\n","\"\"\"\n","suffix =\"\"\"\"\"\"\n","\n","print(get_code_completion(prefix, suffix))"],"metadata":{"id":"29xxp1eHTgJ9"},"execution_count":null,"outputs":[]}]}